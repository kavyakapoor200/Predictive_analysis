{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyakapoor200/Predictive_analysis/blob/main/predictive_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-AMJm89cBWq",
        "outputId": "cb085f3a-a54e-49e5-89ef-03f969cf91a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved as 'big_sample_data.csv'. First 10 rows:\n",
            "   Machine_ID  Temperature  Run_Time  Downtime_Flag\n",
            "0           1           69        65              1\n",
            "1           2           70       420              0\n",
            "2           3           95        78              0\n",
            "3           4           97       388              1\n",
            "4           5           63       409              1\n",
            "5           6           70        78              1\n",
            "6           7           60       103              0\n",
            "7           8           98       391              0\n",
            "8           9           70       304              1\n",
            "9          10           94       192              0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Generate a dataset with 100 rows\n",
        "data = {\n",
        "    \"Machine_ID\": list(range(1, 501)),  # Machine IDs from 1 to 100\n",
        "    \"Temperature\": [random.randint(60, 100) for _ in range(500)],  # Random temperatures between 60 and 100\n",
        "    \"Run_Time\": [random.randint(50, 500) for _ in range(500)],  # Random runtime between 50 and 500 hours\n",
        "    \"Downtime_Flag\": [random.choice([0, 1]) for _ in range(500)]  # Random binary flag (0 or 1)\n",
        "}\n",
        "\n",
        "# Convert it to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"big_sample_data1.csv\", index=False)\n",
        "\n",
        "# Display the first 10 rows of the dataset\n",
        "print(\"Dataset created and saved as 'big_sample_data.csv'. First 10 rows:\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_dazBnFcRfk",
        "outputId": "05b53f66-c056-411c-afb2-5e6d706c7494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and split into training and testing sets.\n",
            "Training samples: 400, Testing samples: 100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"big_sample_data1.csv\")\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[[\"Temperature\", \"Run_Time\"]]\n",
        "y = df[\"Downtime_Flag\"]\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset loaded and split into training and testing sets.\")\n",
        "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x432eNLcYul",
        "outputId": "07ccdd24-6f47-4e6a-d693-422e23bd8883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n",
            "Model features: Index(['Temperature', 'Run_Time'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Model features: {X.columns}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsBGQ9fNcgdw",
        "outputId": "b7cde044-9d24-4002-860d-b38a5bcf5da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation:\n",
            "Accuracy: 0.40\n",
            "F1 Score: 0.57\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Evaluation:\\nAccuracy: {accuracy:.2f}\\nF1 Score: {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsdsGhmFctw3",
        "outputId": "dc7b9753-9bf8-4f4c-e86c-5007e20c1aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import joblib\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "print(\"Model saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB06-VQncuuj",
        "outputId": "c989d7a7-2d91-444a-a91c-3f5c1b0c0ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[[0.52869497 0.47130503]]\n",
            "Prediction: No Downtime, Confidence: 0.53\n"
          ]
        }
      ],
      "source": [
        "# Example: Predict downtime for a new data point\n",
        "new_data = pd.DataFrame({\"Temperature\": [100], \"Run_Time\": [150]})\n",
        "prediction = loaded_model.predict(new_data)\n",
        "print(prediction)\n",
        "probability= loaded_model.predict_proba(new_data)\n",
        "print(probability)\n",
        "confidence = max(loaded_model.predict_proba(new_data)[0])\n",
        "\n",
        "print(f\"Prediction: {'Downtime' if prediction[0] == 1 else 'No Downtime'}, Confidence: {confidence:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHnUGsw5dozp",
        "outputId": "8cbfc1b1-4603-4153-f73d-588e48a1db74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.5)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.41.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn scikit-learn pandas pydantic python-multipart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "bG5ty5oDhuec"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import joblib\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "# Use the lifespan context manager for app startup and shutdown\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    global model\n",
        "    model = None  # Initialize the model as None\n",
        "    try:\n",
        "        # Load the model if available\n",
        "        model = joblib.load(\"logistic_model.pkl\")\n",
        "        print(\"Model loaded successfully.\")\n",
        "        print(\"Expected features:\", model.feature_names_in_)\n",
        "    except FileNotFoundError:\n",
        "        print(\"No pre-trained model found. Train the model first.\")\n",
        "    yield  # Continue with FastAPI's normal lifespan behavior\n",
        "\n",
        "# Create FastAPI app with the lifespan event\n",
        "app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "# Add CORS middleware (optional, useful for frontend integration)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Global variables\n",
        "data = None\n",
        "\n",
        "# Endpoint to upload data\n",
        "@app.post(\"/upload\")\n",
        "async def upload_data(file: UploadFile = File(...)):\n",
        "    global data\n",
        "    if file.content_type != \"text/csv\":\n",
        "        raise HTTPException(status_code=400, detail=\"Only CSV files are accepted.\")\n",
        "\n",
        "    try:\n",
        "        # Read the CSV data\n",
        "        data = pd.read_csv(file.file)\n",
        "        if 'Downtime_Flag' not in data.columns:\n",
        "            raise ValueError(\"The uploaded file must contain a 'Downtime_Flag' column.\")\n",
        "\n",
        "        return {\"message\": \"File uploaded successfully.\", \"data_preview\": data.head().to_dict()}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing file: {str(e)}\")\n",
        "\n",
        "# Endpoint to train the model\n",
        "@app.post(\"/train\")\n",
        "def train_model():\n",
        "    global model, data\n",
        "    if data is None:\n",
        "        raise HTTPException(status_code=400, detail=\"No data uploaded. Please upload data first.\")\n",
        "\n",
        "    try:\n",
        "        X = data[[\"Temperature\", \"Run_Time\"]]\n",
        "        y = data[\"Downtime_Flag\"]\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train a Logistic Regression model\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "        # Save the model\n",
        "        joblib.dump(model, \"logistic_model.pkl\")\n",
        "\n",
        "        return {\"message\": \"Model trained successfully.\", \"accuracy\": accuracy, \"f1_score\": f1}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error during training: {str(e)}\")\n",
        "\n",
        "# Endpoint to make predictions\n",
        "class PredictInput(BaseModel):\n",
        "    Temperature: float\n",
        "    Run_Time: float\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(input: PredictInput):\n",
        "    global model\n",
        "    if model is None:\n",
        "        raise HTTPException(status_code=400, detail=\"Model not trained. Please train the model first.\")\n",
        "\n",
        "    try:\n",
        "        # Prepare the input data\n",
        "        input_data = pd.DataFrame([input.dict()])\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(input_data)[0]\n",
        "        confidence = max(model.predict_proba(input_data)[0])\n",
        "\n",
        "        return {\"Downtime\": \"Yes\" if prediction == 1 else \"No\", \"Confidence\": confidence}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"An error occurred: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpZ-96dbrnko",
        "outputId": "e3f4aa0f-6bf8-4086-df61-20d8ab9f5c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3851]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 8000): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Expected features: ['Temperature' 'Run_Time']\n",
            "FastAPI app is live at: NgrokTunnel: \"https://5152-34-86-232-7.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "# Function to run the FastAPI app using Uvicorn\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "\n",
        "\n",
        "# Start the FastAPI app in a background thread\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()\n",
        "\n",
        "# Set up ngrok to tunnel the app\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"FastAPI app is live at: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install curl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTzIJ6pUE-r",
        "outputId": "419863e8-4177-4058-9c6a-7445b67b84c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 5,484 B/129 kB 4%] [Connected to\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 69.2 kB/129 kB 54%] [2 InRelease\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 124 kB/129 kB 96%] [Connected to\r                                                                                                    \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,561 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,228 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,619 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,646 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,860 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,519 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.6 kB]\n",
            "Fetched 19.9 MB in 2s (8,613 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 64 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "BboK7kNHUKDw",
        "outputId": "f2bb0f20-a816-4c48-f0d5-22ccb9f22141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-58fa61cd-adfc-4b3b-8ffb-6817bbf9f1c6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-58fa61cd-adfc-4b3b-8ffb-6817bbf9f1c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving big_sample_data1.csv to big_sample_data1 (4).csv\n",
            "Uploaded file: big_sample_data1 (4).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using variables in the curl command\n",
        "file_name = \"big_sample_data1.csv\"\n",
        "public_url = \"5152-34-86-232-7.ngrok-free.app\"\n",
        "\n",
        "# Properly formatted curl command with f-string\n",
        "!curl -X POST \"{public_url}/upload\" \\\n",
        "-H \"Content-Type: multipart/form-data\" \\\n",
        "-F \"file=@{file_name};type=text/csv\"\n"
      ],
      "metadata": {
        "id": "IMR-Nh9VUR4q"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST \"5152-34-86-232-7.ngrok-free.app\"\n"
      ],
      "metadata": {
        "id": "oOfJuFjhU_LP"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST \"https://c518-34-86-232-7.ngrok-free.app/predict\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"Temperature\": 75.0, \"Run_Time\": 120.5}'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kovP1-QKVCd_",
        "outputId": "b5c0573a-280d-4072-e19e-d45a1e69f18a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     34.86.232.7:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "{\"Downtime\":\"Yes\",\"Confidence\":0.5407108299161656}"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-e3b58afb32ce>:67: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  input_data = pd.DataFrame([input.dict()])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMBcQMAjI-LS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS9jJJGVxHgx9DCS8zBaKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}